{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb058336-acc2-4b05-bbd9-4e25e6646e0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Die XML-Dateien sind gleich.\n",
      "------------------------------ postprocessed_xml ohne Entity Tags: -------------------------\n",
      "ABl_1981__S__186_.xml\n",
      "------------------------------ original_xml: -------------------------\n",
      "Statistics saved to statistics.csv\n",
      "Processed and saved ABl_1981__S__186_.xml to test_data/postprocessed/ABl_1981__S__186_.xml\n",
      "Die XML-Dateien sind gleich.\n",
      "------------------------------ postprocessed_xml ohne Entity Tags: -------------------------\n",
      "ABl_1980__S__700-704_.xml\n",
      "------------------------------ original_xml: -------------------------\n",
      "Statistics saved to statistics.csv\n",
      "Processed and saved ABl_1980__S__700-704_.xml to test_data/postprocessed/ABl_1980__S__700-704_.xml\n",
      "Die XML-Dateien sind gleich.\n",
      "------------------------------ postprocessed_xml ohne Entity Tags: -------------------------\n",
      "ABl_1982__S__1247-1255_.xml\n",
      "------------------------------ original_xml: -------------------------\n",
      "Statistics saved to statistics.csv\n",
      "Processed and saved ABl_1982__S__1247-1255_.xml to test_data/postprocessed/ABl_1982__S__1247-1255_.xml\n",
      "Die XML-Dateien sind gleich.\n",
      "------------------------------ postprocessed_xml ohne Entity Tags: -------------------------\n",
      "ABl_1982__S__1030-1049_.xml\n",
      "------------------------------ original_xml: -------------------------\n",
      "Statistics saved to statistics.csv\n",
      "Processed and saved ABl_1982__S__1030-1049_.xml to test_data/postprocessed/ABl_1982__S__1030-1049_.xml\n",
      "Die XML-Dateien sind gleich.\n",
      "------------------------------ postprocessed_xml ohne Entity Tags: -------------------------\n",
      "ABl_1983__S__407-415_.xml\n",
      "------------------------------ original_xml: -------------------------\n",
      "Statistics saved to statistics.csv\n",
      "Processed and saved ABl_1983__S__407-415_.xml to test_data/postprocessed/ABl_1983__S__407-415_.xml\n",
      "Die XML-Dateien sind gleich.\n",
      "------------------------------ postprocessed_xml ohne Entity Tags: -------------------------\n",
      "ABl_1980__S__749-754_.xml\n",
      "------------------------------ original_xml: -------------------------\n",
      "Statistics saved to statistics.csv\n",
      "Processed and saved ABl_1980__S__749-754_.xml to test_data/postprocessed/ABl_1980__S__749-754_.xml\n",
      "Die XML-Dateien sind gleich.\n",
      "------------------------------ postprocessed_xml ohne Entity Tags: -------------------------\n",
      "ABl_1982__S__917-922_.xml\n",
      "------------------------------ original_xml: -------------------------\n",
      "Statistics saved to statistics.csv\n",
      "Processed and saved ABl_1982__S__917-922_.xml to test_data/postprocessed/ABl_1982__S__917-922_.xml\n",
      "Die XML-Dateien sind gleich.\n",
      "------------------------------ postprocessed_xml ohne Entity Tags: -------------------------\n",
      "ABl_1981__S__1907-1910_.xml\n",
      "------------------------------ original_xml: -------------------------\n",
      "Statistics saved to statistics.csv\n",
      "Processed and saved ABl_1981__S__1907-1910_.xml to test_data/postprocessed/ABl_1981__S__1907-1910_.xml\n",
      "Die XML-Dateien sind gleich.\n",
      "------------------------------ postprocessed_xml ohne Entity Tags: -------------------------\n",
      "ABl_1981__S__295-296_.xml\n",
      "------------------------------ original_xml: -------------------------\n",
      "Statistics saved to statistics.csv\n",
      "Processed and saved ABl_1981__S__295-296_.xml to test_data/postprocessed/ABl_1981__S__295-296_.xml\n",
      "Die XML-Dateien sind gleich.\n",
      "------------------------------ postprocessed_xml ohne Entity Tags: -------------------------\n",
      "ABl_1981__S__934-958_.xml\n",
      "------------------------------ original_xml: -------------------------\n",
      "Statistics saved to statistics.csv\n",
      "Processed and saved ABl_1981__S__934-958_.xml to test_data/postprocessed/ABl_1981__S__934-958_.xml\n",
      "Die XML-Dateien sind gleich.\n",
      "------------------------------ postprocessed_xml ohne Entity Tags: -------------------------\n",
      "ABl_1981__S__420-422_.xml\n",
      "------------------------------ original_xml: -------------------------\n",
      "Statistics saved to statistics.csv\n",
      "Processed and saved ABl_1981__S__420-422_.xml to test_data/postprocessed/ABl_1981__S__420-422_.xml\n",
      "Die XML-Dateien sind gleich.\n",
      "------------------------------ postprocessed_xml ohne Entity Tags: -------------------------\n",
      "ABl_1982__S__15-24_.xml\n",
      "------------------------------ original_xml: -------------------------\n",
      "Statistics saved to statistics.csv\n",
      "Processed and saved ABl_1982__S__15-24_.xml to test_data/postprocessed/ABl_1982__S__15-24_.xml\n",
      "Die XML-Dateien sind gleich.\n",
      "------------------------------ postprocessed_xml ohne Entity Tags: -------------------------\n",
      "ABl_1981__S__193-204_.xml\n",
      "------------------------------ original_xml: -------------------------\n",
      "Statistics saved to statistics.csv\n",
      "Processed and saved ABl_1981__S__193-204_.xml to test_data/postprocessed/ABl_1981__S__193-204_.xml\n",
      "Die XML-Dateien sind gleich.\n",
      "------------------------------ postprocessed_xml ohne Entity Tags: -------------------------\n",
      "ABl_1981__S__511-514_.xml\n",
      "------------------------------ original_xml: -------------------------\n",
      "Statistics saved to statistics.csv\n",
      "Processed and saved ABl_1981__S__511-514_.xml to test_data/postprocessed/ABl_1981__S__511-514_.xml\n",
      "Die XML-Dateien sind gleich.\n",
      "------------------------------ postprocessed_xml ohne Entity Tags: -------------------------\n",
      "ABl_1982__S__278-287_.xml\n",
      "------------------------------ original_xml: -------------------------\n",
      "Statistics saved to statistics.csv\n",
      "Processed and saved ABl_1982__S__278-287_.xml to test_data/postprocessed/ABl_1982__S__278-287_.xml\n",
      "Die XML-Dateien sind gleich.\n",
      "------------------------------ postprocessed_xml ohne Entity Tags: -------------------------\n",
      "ABl_1983__S__364__Eintrag_1_.xml\n",
      "------------------------------ original_xml: -------------------------\n",
      "Statistics saved to statistics.csv\n",
      "Processed and saved ABl_1983__S__364__Eintrag_1_.xml to test_data/postprocessed/ABl_1983__S__364__Eintrag_1_.xml\n",
      "Die XML-Dateien sind gleich.\n",
      "------------------------------ postprocessed_xml ohne Entity Tags: -------------------------\n",
      "ABl_1981__S__698-699_.xml\n",
      "------------------------------ original_xml: -------------------------\n",
      "Statistics saved to statistics.csv\n",
      "Processed and saved ABl_1981__S__698-699_.xml to test_data/postprocessed/ABl_1981__S__698-699_.xml\n",
      "Die XML-Dateien sind gleich.\n",
      "------------------------------ postprocessed_xml ohne Entity Tags: -------------------------\n",
      "ABl_1981__S__986-987_.xml\n",
      "------------------------------ original_xml: -------------------------\n",
      "Statistics saved to statistics.csv\n",
      "Processed and saved ABl_1981__S__986-987_.xml to test_data/postprocessed/ABl_1981__S__986-987_.xml\n",
      "Die XML-Dateien sind gleich.\n",
      "------------------------------ postprocessed_xml ohne Entity Tags: -------------------------\n",
      "ABl_1980__S__1263-1264_.xml\n",
      "------------------------------ original_xml: -------------------------\n",
      "Statistics saved to statistics.csv\n",
      "Processed and saved ABl_1980__S__1263-1264_.xml to test_data/postprocessed/ABl_1980__S__1263-1264_.xml\n",
      "Die XML-Dateien sind gleich.\n",
      "------------------------------ postprocessed_xml ohne Entity Tags: -------------------------\n",
      "ABl_1980__S__634_.xml\n",
      "------------------------------ original_xml: -------------------------\n",
      "Statistics saved to statistics.csv\n",
      "Processed and saved ABl_1980__S__634_.xml to test_data/postprocessed/ABl_1980__S__634_.xml\n",
      "Die XML-Dateien sind gleich.\n",
      "------------------------------ postprocessed_xml ohne Entity Tags: -------------------------\n",
      "ABl_1980__S__220-221_.xml\n",
      "------------------------------ original_xml: -------------------------\n",
      "Statistics saved to statistics.csv\n",
      "Processed and saved ABl_1980__S__220-221_.xml to test_data/postprocessed/ABl_1980__S__220-221_.xml\n",
      "Die XML-Dateien sind gleich.\n",
      "------------------------------ postprocessed_xml ohne Entity Tags: -------------------------\n",
      "ABl_1981__S__1447-1450_.xml\n",
      "------------------------------ original_xml: -------------------------\n",
      "Statistics saved to statistics.csv\n",
      "Processed and saved ABl_1981__S__1447-1450_.xml to test_data/postprocessed/ABl_1981__S__1447-1450_.xml\n",
      "Die XML-Dateien sind gleich.\n",
      "------------------------------ postprocessed_xml ohne Entity Tags: -------------------------\n",
      "ABl_1982__S__106-127_.xml\n",
      "------------------------------ original_xml: -------------------------\n",
      "Statistics saved to statistics.csv\n",
      "Processed and saved ABl_1982__S__106-127_.xml to test_data/postprocessed/ABl_1982__S__106-127_.xml\n",
      "Die XML-Dateien sind gleich.\n",
      "------------------------------ postprocessed_xml ohne Entity Tags: -------------------------\n",
      "ABl_1983__S__153-164_.xml\n",
      "------------------------------ original_xml: -------------------------\n",
      "Statistics saved to statistics.csv\n",
      "Processed and saved ABl_1983__S__153-164_.xml to test_data/postprocessed/ABl_1983__S__153-164_.xml\n",
      "Die XML-Dateien sind gleich.\n",
      "------------------------------ postprocessed_xml ohne Entity Tags: -------------------------\n",
      "ABl_1981__S__1533-1556_.xml\n",
      "------------------------------ original_xml: -------------------------\n",
      "Statistics saved to statistics.csv\n",
      "Processed and saved ABl_1981__S__1533-1556_.xml to test_data/postprocessed/ABl_1981__S__1533-1556_.xml\n",
      "Die XML-Dateien sind gleich.\n",
      "------------------------------ postprocessed_xml ohne Entity Tags: -------------------------\n",
      "ABl_1980__S__310-312_.xml\n",
      "------------------------------ original_xml: -------------------------\n",
      "Statistics saved to statistics.csv\n",
      "Processed and saved ABl_1980__S__310-312_.xml to test_data/postprocessed/ABl_1980__S__310-312_.xml\n",
      "Die XML-Dateien sind gleich.\n",
      "------------------------------ postprocessed_xml ohne Entity Tags: -------------------------\n",
      "ABl_1981__S__1010-1012_.xml\n",
      "------------------------------ original_xml: -------------------------\n",
      "Statistics saved to statistics.csv\n",
      "Processed and saved ABl_1981__S__1010-1012_.xml to test_data/postprocessed/ABl_1981__S__1010-1012_.xml\n",
      "Die XML-Dateien sind gleich.\n",
      "------------------------------ postprocessed_xml ohne Entity Tags: -------------------------\n",
      "ABl_1981__S__692-693_.xml\n",
      "------------------------------ original_xml: -------------------------\n",
      "Statistics saved to statistics.csv\n",
      "Processed and saved ABl_1981__S__692-693_.xml to test_data/postprocessed/ABl_1981__S__692-693_.xml\n",
      "Die XML-Dateien sind gleich.\n",
      "------------------------------ postprocessed_xml ohne Entity Tags: -------------------------\n",
      "ABl_1982__S__733-742_.xml\n",
      "------------------------------ original_xml: -------------------------\n",
      "Statistics saved to statistics.csv\n",
      "Processed and saved ABl_1982__S__733-742_.xml to test_data/postprocessed/ABl_1982__S__733-742_.xml\n",
      "Die XML-Dateien sind gleich.\n",
      "------------------------------ postprocessed_xml ohne Entity Tags: -------------------------\n",
      "ABl_1980__S__28_.xml\n",
      "------------------------------ original_xml: -------------------------\n",
      "Statistics saved to statistics.csv\n",
      "Processed and saved ABl_1980__S__28_.xml to test_data/postprocessed/ABl_1980__S__28_.xml\n",
      "Die XML-Dateien sind gleich.\n",
      "------------------------------ postprocessed_xml ohne Entity Tags: -------------------------\n",
      "ABl_1983__S__210-220_.xml\n",
      "------------------------------ original_xml: -------------------------\n",
      "Statistics saved to statistics.csv\n",
      "Processed and saved ABl_1983__S__210-220_.xml to test_data/postprocessed/ABl_1983__S__210-220_.xml\n",
      "Die XML-Dateien sind gleich.\n",
      "------------------------------ postprocessed_xml ohne Entity Tags: -------------------------\n",
      "ABl_1982__S__1234-1235_.xml\n",
      "------------------------------ original_xml: -------------------------\n",
      "Statistics saved to statistics.csv\n",
      "Processed and saved ABl_1982__S__1234-1235_.xml to test_data/postprocessed/ABl_1982__S__1234-1235_.xml\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 487\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll files processed\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    486\u001b[0m \u001b[38;5;66;03m# Let the magic happen\u001b[39;00m\n\u001b[0;32m--> 487\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[4], line 471\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    468\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m    470\u001b[0m \u001b[38;5;66;03m# Merge entities from the edited file into the original file\u001b[39;00m\n\u001b[0;32m--> 471\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mmerge_entities\u001b[49m\u001b[43m(\u001b[49m\u001b[43moriginal_xml\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medited_xml\u001b[49m\u001b[43m)\u001b[49m           \n\u001b[1;32m    473\u001b[0m \u001b[38;5;66;03m# Validate result\u001b[39;00m\n\u001b[1;32m    474\u001b[0m validate_result(filename, original_xml, edited_xml, result)\n",
      "Cell \u001b[0;32mIn[4], line 404\u001b[0m, in \u001b[0;36mmerge_entities\u001b[0;34m(original_xml, edited_xml)\u001b[0m\n\u001b[1;32m    399\u001b[0m context_pattern \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    400\u001b[0m     \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m(?<=\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m re\u001b[38;5;241m.\u001b[39mescape(text_for_lookbehind) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms*\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m re\u001b[38;5;241m.\u001b[39mescape(searchText)\n\u001b[1;32m    401\u001b[0m )\n\u001b[1;32m    403\u001b[0m \u001b[38;5;66;03m# Perform the replacement if the lookbehind is found\u001b[39;00m\n\u001b[0;32m--> 404\u001b[0m original_body_str, count \u001b[38;5;241m=\u001b[39m \u001b[43mre\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontext_pattern\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreplaceText\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moriginal_body_str\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcount\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    406\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m count \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    407\u001b[0m     \u001b[38;5;66;03m# If no replacements were made, add the entity to the unreplaced list\u001b[39;00m\n\u001b[1;32m    408\u001b[0m     unreplaced_entities\u001b[38;5;241m.\u001b[39mappend(entity)\n",
      "File \u001b[0;32m/usr/local/python/3.12.1/lib/python3.12/re/__init__.py:197\u001b[0m, in \u001b[0;36msubn\u001b[0;34m(pattern, repl, string, count, flags)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msubn\u001b[39m(pattern, repl, string, count\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, flags\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m):\n\u001b[1;32m    189\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return a 2-tuple containing (new_string, number).\u001b[39;00m\n\u001b[1;32m    190\u001b[0m \u001b[38;5;124;03m    new_string is the string obtained by replacing the leftmost\u001b[39;00m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;124;03m    non-overlapping occurrences of the pattern in the source\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;124;03m    If it is a callable, it's passed the Match object and must\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;124;03m    return a replacement string to be used.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_compile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpattern\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrepl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstring\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcount\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\" Script for correcting unintended text modifications during Named Entity Recognition (NER) \n",
    "\n",
    "This script addresses issues that may arise during the NER process \n",
    "according to the TEI standard, mostly unintended text duplications. \n",
    "An example of such a modification is:\n",
    ">>> (...) Seminar der Univer<lb break=\"no\" facs=\"#facs_290_r33\"/>sität</orgName>sität</cell>\n",
    "The script corrects this modification to:\n",
    ">>> (...) Seminar der Univer<lb break=\"no\" facs=\"#facs_290_r33\"/>sität</orgName></cell>\n",
    "\n",
    "The NER process checks the documents and detects errors. Documents with \n",
    "errors are saved in an /error folder. This script takes the NER-processed \n",
    "files with errors (hereafter referred to as the \"edited file\")  and extracts \n",
    "all TEI entities with some context. It then inserts the entities into the \n",
    "original file (before NER) using search and replace.\n",
    "\n",
    "The script accepts XML files (.xml) as input.\n",
    "\n",
    "Requirements:\n",
    "- This script requires that `beautifulsoup4` and `lxml` are installed in the Python environment where you are running this script.\n",
    "\n",
    "Installation of beautifulsoup4:\n",
    "- To install beautifulsoup4, run the following command in your command line:\n",
    "  ```bash\n",
    "  pip install beautifulsoup4\n",
    "\n",
    "Installation of lxml:\n",
    "- To install lxml, run the following command in your command line:\n",
    "  ```bash\n",
    "  pip install lxml\n",
    "\"\"\"\n",
    "\n",
    "# pip install beautifulsoup4\n",
    "# pip install lxml\n",
    "import os\n",
    "import re\n",
    "import copy\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "import stats\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from lxml import etree\n",
    "\n",
    "# Paths for the various directories\n",
    "edited_dir = 'test_data/TEI-XML_NER/error/Amtsblatt/' # Directory containing NER-processed files with errors\n",
    "original_dir = 'test_data/TEI-XML/Amtsblatt/' # Directory containing the original files\n",
    "output_dir = 'test_data/postprocessed/' # Output directory for the merged files generated by this script\n",
    "\n",
    "\n",
    "\n",
    "def is_nested(entity):\n",
    "    \"\"\" Check if the current entity is nested within another entity.\n",
    "\n",
    "    Args:\n",
    "        entity (BeautifulSoup Tag): The entity (e.g., <placeName>, <persName>, <orgName>).\n",
    "\n",
    "    Returns:\n",
    "        bool: True if the entity is nested within another entity, False otherwise.\n",
    "    \"\"\"\n",
    "    \n",
    "    parent = entity.find_parent()\n",
    "    while parent:\n",
    "        if parent.name in {'placeName', 'persName', 'orgName'} and parent != entity:\n",
    "            return True\n",
    "        parent = parent.find_parent()\n",
    "    return False\n",
    "\n",
    "\n",
    "def filter_nested_entities(entities):\n",
    "    \"\"\" Remove all nested entities from the list that are already contained within a parent entity\n",
    "\n",
    "    Args:\n",
    "        entities (list of BeautifulSoup Tags): List of entity tags from the edited XML.\n",
    "\n",
    "    Returns:\n",
    "        list of BeautifulSoup Tags: List of non-nested entities.\n",
    "    \"\"\"\n",
    "    \n",
    "    non_nested_entities = []\n",
    "\n",
    "    for entity in entities:\n",
    "        if not is_nested(entity):\n",
    "            non_nested_entities.append(entity)\n",
    "\n",
    "    return non_nested_entities\n",
    "\n",
    "def count_entities(text):\n",
    "    total_count  = 0\n",
    "\n",
    "    tag_patterns = {\n",
    "        'placeName': r'</?placeName[^>]*>',\n",
    "        'persName': r'</?persName[^>]*>',\n",
    "        'orgName': r'</?orgName[^>]*>'\n",
    "    }\n",
    "\n",
    "    # Durchlaufe jedes Pattern und summiere die Anzahl der Vorkommen\n",
    "    for pattern in tag_patterns.values():\n",
    "        total_count  += len(re.findall(pattern, text))\n",
    "\n",
    "    # Dividiere durch 2, da es immer ein Start- und Endtag gibt. Es werden Start- und Endtags gezählt, um zu erkennen, falls nur einer von beiden fehlt.\n",
    "    return (total_count//2)\n",
    "    \n",
    "\n",
    "def remove_entity_tags_in_str(text):\n",
    "    \"\"\" Remove all TEI entity tags (placeName, persName, orgName) from a string.\n",
    "\n",
    "    Args:\n",
    "        text (str): The input string that may contain TEI entity tags.\n",
    "\n",
    "    Returns:\n",
    "        str: The input string with all TEI entity tags removed.\n",
    "    \"\"\"\n",
    "    \n",
    "    tag_patterns = {\n",
    "        'placeName': r'</?placeName[^>]*>',\n",
    "        'persName': r'</?persName[^>]*>',\n",
    "        'orgName': r'</?orgName[^>]*>'\n",
    "    }\n",
    "    \n",
    "    # Remove the tags for each entity type\n",
    "    for tag_name, pattern in tag_patterns.items():\n",
    "        text = re.sub(pattern, '', text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "\n",
    "def get_text_for_lookbehind(entity, removeEntityTags = True):\n",
    "    \"\"\" Extract up to 30 characters of text before the entity, used for regex lookbehind.\n",
    "\n",
    "    Args:\n",
    "        entity (BeautifulSoup Tag): The entity tag from which to extract the lookbehind text.\n",
    "        removeEntityTags (bool, optional): Whether to remove nested entity tags from the lookbehind text. Defaults to True.\n",
    "\n",
    "    Returns:\n",
    "        str: The 30 characters (if removeEntityTags is false) or 20 characters (otherwise) before the entity in the parent element.\n",
    "    \"\"\"\n",
    "    \n",
    "    parent_element = entity.find_parent()\n",
    "    \n",
    "    if parent_element:\n",
    "        parent_text = ''.join(str(content) for content in parent_element.contents)\n",
    "        entity_str = str(entity)\n",
    "        index_of_child = parent_text.find(entity_str)\n",
    "        text_before_child_with_entities = parent_text[:index_of_child]\n",
    "\n",
    "        if not removeEntityTags:\n",
    "            # return 20 characters before entity tag as lookbehind text\n",
    "            return text_before_child_with_entities[-20:]\n",
    "\n",
    "        text_before_child_without_entities = remove_entity_tags_in_str(text_before_child_with_entities)\n",
    "\n",
    "        # return 30 characters before entity tag as lookbehind text\n",
    "        return text_before_child_without_entities[-30:]\n",
    "\n",
    "    return \"\"\n",
    "    \n",
    "\n",
    "def prepare_search_text(entity):\n",
    "    \"\"\" Prepare the search text by removing all entity tags from the entity. \n",
    "        This will make the text match the text in the original file (before NER).\n",
    "\n",
    "    Args:\n",
    "        entity (BeautifulSoup Tag): The entity whose text is being prepared.\n",
    "\n",
    "    Returns:\n",
    "        str: The cleaned entity text, without any nested tags.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Remove all nested entities inside the current entity\n",
    "    for inner_entity in entity.find_all(['placeName', 'persName', 'orgName']):\n",
    "        inner_entity.unwrap()  # Removes the tag but retains the content\n",
    "\n",
    "    # Also remove the parent entity to prepare the text for search\n",
    "    search_text = ''.join(str(content) for content in entity.contents)\n",
    "    \n",
    "    return search_text\n",
    "\n",
    "    \n",
    "def insert_done_in_every_word(sentence):\n",
    "    \"\"\" Insert the marker `---DONE---` into each word of the replacement string\n",
    "    to prevent repeated matches during the search-and-replace process.\n",
    "\n",
    "    Args:\n",
    "        sentence (str): The sentence or entity text in which to insert the marker.\n",
    "\n",
    "    Returns:\n",
    "        str: The modified sentence with `---DONE---` inserted.\n",
    "    \"\"\"\n",
    "    \n",
    "    modified_words = []\n",
    "\n",
    "    pattern = re.compile(r'(<[^>]*>| )')\n",
    "    words = pattern.split(sentence)\n",
    "    \n",
    "    for word in words:\n",
    "        modified_word = word[:len(word)//2] + \"---DONE---\" + word[len(word)//2:]\n",
    "        modified_words.append(modified_word)\n",
    "    \n",
    "    modified_sentence = ''.join(modified_words)\n",
    "    \n",
    "    return modified_sentence\n",
    "\n",
    "\n",
    "def validate_result(filename, original_xml, edited_xml, postprocessed_xml):\n",
    "\n",
    "\n",
    "    # Überprüfen, ob kein Inhalt verloren gegangen ist -> Postprocessed ohne Entity-Tags und Whitespaces\n",
    "    # Postprocessed <-> Original\n",
    "\n",
    "    def normalize_xml(xml_string):\n",
    "        root = ET.fromstring(xml_string)\n",
    "    \n",
    "        def sort_attributes(elem):\n",
    "            elem.attrib = {k: v for k, v in sorted(elem.attrib.items())}\n",
    "            \n",
    "            for child in elem:\n",
    "                sort_attributes(child)\n",
    "    \n",
    "        sort_attributes(root)\n",
    "    \n",
    "        # Entferne überflüssige Whitespaces\n",
    "        normalized_string = ET.tostring(root, encoding='utf-8').decode('utf-8')\n",
    "        normalized_string = re.sub(r\">\\s+<\", \"><\", normalized_string)\n",
    "        normalized_string = re.sub(r\"\\s+\", \" \", normalized_string)\n",
    "        \n",
    "        return normalized_string\n",
    "    \n",
    "    def compare_xml_strings(xml_string1, xml_string2):\n",
    "        # Normalisiere beide XML-Strings\n",
    "        normalized_xml1 = normalize_xml(xml_string1)\n",
    "        normalized_xml2 = normalize_xml(xml_string2)\n",
    "    \n",
    "        # Vergleiche die beiden normalisierten XML-Strings\n",
    "        if normalized_xml1 == normalized_xml2:\n",
    "            print(\"Die XML-Dateien sind gleich.\")\n",
    "            stats.write_to_statistics(filename,'Content integrity (Original <-> Postprocessed)', 'Yes')\n",
    "        else:\n",
    "            print(\"Die XML-Dateien sind unterschiedlich.\")\n",
    "            stats.write_to_statistics(filename,'Content integrity (Original <-> Postprocessed)', 'No')\n",
    "\n",
    "    compare_xml_strings(original_xml, remove_entity_tags_in_str(postprocessed_xml))\n",
    "\n",
    "    print(\"------------------------------ postprocessed_xml ohne Entity Tags: -------------------------\")\n",
    "    print(filename)\n",
    "    #print(normalize_xml(remove_entity_tags_in_str(postprocessed_xml)))\n",
    "    print(\"------------------------------ original_xml: -------------------------\")\n",
    "    #print(filename)\n",
    "    #print(normalize_xml(original_xml))\n",
    "\n",
    "    # Vlt auch Differenzen zwischen postprocessed <-> edited im csv ausgeben\n",
    "    \n",
    "\n",
    "    # XML Validieren ob Syntax korrekt ist.\n",
    "\n",
    "    # Kann geprüft werden, ob alle Fehler korrigiert wurden?\n",
    "\n",
    "    # Für Statistik zählen, wie viele Entities ersetzt wurden\n",
    "    entities_before_postprocessing = count_entities(postprocessed_xml)\n",
    "    entities_after_postprocessing = count_entities(edited_xml)\n",
    "    stats.write_to_statistics(filename,'Number of Entities After Processing', entities_before_postprocessing)\n",
    "    stats.write_to_statistics(filename,'Number of Entities Before Processing', entities_after_postprocessing)\n",
    "\n",
    "    # Prüfen, ob die Anzahl Entitäten übereinstimmt\n",
    "    # error <-> postprocessed\n",
    "    missing_entities = entities_after_postprocessing - entities_before_postprocessing\n",
    "    stats.write_to_statistics(filename,'Missing Entities', missing_entities)\n",
    "\n",
    "\n",
    "    '''\n",
    "    # Remove all entities from postprocessed_xml\n",
    "    postprocessed_xml = remove_entity_tags_in_str(postprocessed_xml)\n",
    "\n",
    "    if original_xml.startswith('<?xml'):\n",
    "        original_xml = original_xml.split('?>', 1)[1].strip()\n",
    "\n",
    "    if postprocessed_xml.startswith('<?xml'):\n",
    "        postprocessed_xml = postprocessed_xml.split('?>', 1)[1].strip()\n",
    "    \n",
    "    def normalize_xml(xml_content):\n",
    "        \"\"\"Parses the XML content and returns a normalized form.\"\"\"\n",
    "        \n",
    "        # Encode the string to bytes\n",
    "        #xml_bytes = xml_content.encode('utf-8')\n",
    "        #root = etree.fromstring(xml_bytes)\n",
    "        root = etree.fromstring(xml_content)\n",
    "    \n",
    "        # Normalize the XML by sorting the attributes\n",
    "        def sort_attributes(element):\n",
    "            # Create a new element with sorted attributes\n",
    "            sorted_attrib = dict(sorted(element.attrib.items()))\n",
    "            new_element = etree.Element(element.tag, **sorted_attrib)\n",
    "    \n",
    "            # Copy the text and children\n",
    "            new_element.text = element.text\n",
    "            for child in element:\n",
    "                new_child = sort_attributes(child)  # Recursively sort child attributes\n",
    "                new_element.append(new_child)\n",
    "    \n",
    "            return new_element\n",
    "    \n",
    "        # Create a new root element with sorted attributes\n",
    "        normalized_root = sort_attributes(root)\n",
    "    \n",
    "        # Convert the normalized XML to a string\n",
    "        return etree.tostring(normalized_root, pretty_print=True, encoding='utf-8')\n",
    "    \n",
    "    def compare_xml_content(file1, file2):\n",
    "        \"\"\"Compares the content of two XML files and returns a score.\"\"\"\n",
    "        normalized_xml1 = normalize_xml(file1)\n",
    "        normalized_xml2 = normalize_xml(file2)\n",
    "\n",
    "        print(\"------------------------------ Normalized 1: -------------------------\")\n",
    "        #print(normalized_xml1)\n",
    "        print(\"------------------------------ Normalized 2: -------------------------\")\n",
    "        #print(normalized_xml2)\n",
    "    \n",
    "        # Count the number of matching elements\n",
    "        tree1 = etree.fromstring(normalized_xml1)\n",
    "        tree2 = etree.fromstring(normalized_xml2)\n",
    "\n",
    "        tree1 = tree1.replace('\\t', '')\n",
    "        tree2 = tree2.replace('\\t', '')\n",
    "    \n",
    "        def count_matching_elements(elem1, elem2):\n",
    "            \"\"\"Counts the matching elements and their text contents.\"\"\"\n",
    "            count = 0\n",
    "            total = 0\n",
    "    \n",
    "            # Compare the elements\n",
    "            for child1 in elem1:\n",
    "                total += 1\n",
    "                for child2 in elem2:\n",
    "                    if child1.tag == child2.tag and child1.text == child2.text:\n",
    "                        count += 1\n",
    "                        break\n",
    "    \n",
    "            return count, total\n",
    "    \n",
    "        matching_count, total_count = count_matching_elements(tree1, tree2)\n",
    "    \n",
    "        # Calculate the score\n",
    "        if total_count == 0:\n",
    "            return 0.0  # Avoid division by zero\n",
    "        score = matching_count / total_count\n",
    "        return score\n",
    "    \n",
    "    #score = compare_xml_content(original_xml, postprocessed_xml)\n",
    "    #print(f\"Matching Score: {score:.2f}\")\n",
    "    #return score\n",
    "\n",
    "    '''\n",
    "    \n",
    "def merge_entities(original_xml, edited_xml):\n",
    "    \"\"\" Merge the named entities from the edited XML into the original XML (only within <body>).\n",
    "\n",
    "    Args:\n",
    "        original_xml (str): The XML content of the original file.\n",
    "        edited_xml (str): The XML content of the NER-processed file with errors.\n",
    "\n",
    "    Returns:\n",
    "        str: The original XML content with the corrected entities inserted.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Parse the original and edited XML\n",
    "    original_soup = BeautifulSoup(original_xml, 'xml')\n",
    "    edited_soup = BeautifulSoup(edited_xml, 'xml')\n",
    "\n",
    "    # Extract <body> content from both documents\n",
    "    original_body = original_soup.find('body')\n",
    "    edited_body = edited_soup.find('body')\n",
    "\n",
    "    # Ensure <body> exists in both documents\n",
    "    if original_body and edited_body:\n",
    "        \n",
    "        # Find all entities in the edited XML (places, people, organizations)\n",
    "        entities = edited_body.find_all(['placeName', 'persName', 'orgName'])\n",
    "\n",
    "        # Remove all entities that are already nested within another entity\n",
    "        non_nested_entities = filter_nested_entities(entities)\n",
    "\n",
    "        # List to store entities that couldn't be replaced in the round one\n",
    "        unreplaced_entities = []\n",
    "        \n",
    "        original_body_str = str(original_body)\n",
    "\n",
    "        # Round one using greater context (lookbehind) and ignoring entity tags   \n",
    "        for entity in non_nested_entities:\n",
    "            \n",
    "            searchText = prepare_search_text(copy.deepcopy(entity))\n",
    "            text_for_lookbehind = get_text_for_lookbehind(entity)\n",
    "            \n",
    "            # Insert \"---DONE---\" in replacement text to prevent re-matching\n",
    "            replaceText = insert_done_in_every_word(str(entity))\n",
    "\n",
    "            # Create the regex pattern for contextual replacement\n",
    "            context_pattern = (\n",
    "                r'(?<=' + re.escape(text_for_lookbehind) + r')\\s*' + re.escape(searchText)\n",
    "            )\n",
    "            \n",
    "            # Perform the replacement if the lookbehind is found\n",
    "            original_body_str, count = re.subn(context_pattern, replaceText, original_body_str, count=1)\n",
    "\n",
    "            if count == 0:\n",
    "                # If no replacements were made, add the entity to the unreplaced list\n",
    "                unreplaced_entities.append(entity)\n",
    "\n",
    "        # Remove \"---DONE---\" markers\n",
    "        original_body_str = original_body_str.replace(\"---DONE---\", \"\")\n",
    "\n",
    "\n",
    "        # Round two uses a shorter context and considers the presence of entity tags. \n",
    "        # A possible reason for the previous lookbehind not matching could be that an entity \n",
    "        # has already been inserted into the original file, causing the current lookbehind \n",
    "        # to fail when trying to match with the next entity's surrounding text.\n",
    "        for entity in unreplaced_entities:\n",
    "            \n",
    "            searchText = prepare_search_text(copy.deepcopy(entity))\n",
    "            text_for_lookbehind = get_text_for_lookbehind(entity, removeEntityTags = False)\n",
    "            \n",
    "            # Do not insert \"---DONE---\" in this round. The purpose of the second round is to find entities \n",
    "            # that were missed in the first round due to already inserted entity tags in the original document.\n",
    "            replaceText = str(entity)\n",
    "\n",
    "            context_pattern = (\n",
    "                r'(?<=' + re.escape(text_for_lookbehind) + r')\\s*' + re.escape(searchText)\n",
    "            )\n",
    "    \n",
    "            original_body_str = re.sub(context_pattern, replaceText, original_body_str, count=1)\n",
    "            \n",
    "\n",
    "        # Replace the old <body> with the modified version in the original document\n",
    "        original_body.replace_with(BeautifulSoup(original_body_str, 'xml').body)\n",
    "        \n",
    "    return str(original_soup)\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\" Process XML documents by reading, merging entities, and saving the results.\n",
    "\n",
    "    This function reads NER-processed XML files,\n",
    "    merges the identified entities into the corresponding original files,\n",
    "    and then saves the merged output into a new directory.\n",
    "    \"\"\"\n",
    "\n",
    "    # Ensure the output directory exists, create it if necessary\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    \n",
    "    for filename in os.listdir(edited_dir):\n",
    "        \n",
    "        if filename.endswith('.xml'):\n",
    "\n",
    "            # Read edited and original xml file\n",
    "            edited_file_path = os.path.join(edited_dir, filename)\n",
    "            original_file_path = os.path.join(original_dir, filename)\n",
    "    \n",
    "            with open(edited_file_path, 'r', encoding='utf-8') as file:\n",
    "                edited_xml = file.read()\n",
    "    \n",
    "            if os.path.exists(original_file_path): \n",
    "                with open(original_file_path, 'r', encoding='utf-8') as file:\n",
    "                    original_xml = file.read()\n",
    "            else:\n",
    "                print(f\"Original file not found for {filename}\")\n",
    "                continue\n",
    "    \n",
    "            # Merge entities from the edited file into the original file\n",
    "            result = merge_entities(original_xml, edited_xml)           \n",
    "\n",
    "            # Validate result\n",
    "            validate_result(filename, original_xml, edited_xml, result)\n",
    "    \n",
    "            # Save the merged result to the output directory as an XML document\n",
    "            output_file_path = os.path.join(output_dir, filename)\n",
    "            with open(output_file_path, 'w', encoding='utf-8') as output_file:\n",
    "                output_file.write(result)\n",
    "\n",
    "            stats.save_statistics_to_csv()\n",
    "            print(f\"Processed and saved {filename} to {output_file_path}\")\n",
    "\n",
    "    print(f\"All files processed\")\n",
    "\n",
    "# Let the magic happen\n",
    "main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6d613a6f-a4f8-4da7-b741-142251e7bb83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'stats' from '/workspaces/postprocessing_NER/stats.py'>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib  # Importiere importlib\n",
    "import stats  # Importiere dein Modul\n",
    "\n",
    "# Änderungen an my_module vornehmen...\n",
    "\n",
    "# Jetzt das Modul neu laden\n",
    "importlib.reload(stats)  # Lade das Modul neu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e12137-12e4-4112-bb73-0f7b1ff89793",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
