{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb058336-acc2-4b05-bbd9-4e25e6646e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Script for correcting unintended text modifications during Named Entity Recognition (NER) \n",
    "\n",
    "This script addresses issues that may arise during the NER process \n",
    "according to the TEI standard, mostly unintended text duplications. \n",
    "An example of such a modification is:\n",
    ">>> (...) Seminar der Univer<lb break=\"no\" facs=\"#facs_290_r33\"/>sität</orgName>sität</cell>\n",
    "The script corrects this modification to:\n",
    ">>> (...) Seminar der Univer<lb break=\"no\" facs=\"#facs_290_r33\"/>sität</orgName></cell>\n",
    "\n",
    "The NER process checks the documents and detects errors. Documents with \n",
    "errors are saved in an /error folder. This script takes the NER-processed \n",
    "files with errors (hereafter referred to as the \"edited file\")  and extracts \n",
    "all TEI entities with some context. It then inserts the entities into the \n",
    "original file (before NER) using search and replace.\n",
    "\n",
    "The script accepts XML files (.xml) as input.\n",
    "\n",
    "Requirements:\n",
    "- This script requires that `beautifulsoup4`, `lxml` and `xmlschema` are installed in the Python environment where you are running this script.\n",
    "\n",
    "Installation of beautifulsoup4:\n",
    "- To install beautifulsoup4, run the following command in your command line:\n",
    "  ```bash\n",
    "  pip install beautifulsoup4\n",
    "\n",
    "Installation of lxml:\n",
    "- To install lxml, run the following command in your command line:\n",
    "  ```bash\n",
    "  pip install lxml\n",
    "\n",
    "Installation of xmlschema:\n",
    "- To install xmlschema, run the following command in your command line:\n",
    "  ```bash\n",
    "  pip install xmlschema\n",
    "\"\"\"\n",
    "\n",
    "# pip install beautifulsoup4\n",
    "# pip install lxml\n",
    "# pip install xmlschema\n",
    "\n",
    "import os\n",
    "import re\n",
    "import copy\n",
    "import difflib\n",
    "import xmlschema\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "import stats\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from lxml import etree\n",
    "\n",
    "# Paths for the various directories\n",
    "edited_dir = 'test_data/TEI-XML_NER/error/Amtsblatt/' # Directory containing NER-processed files with errors\n",
    "original_dir = 'test_data/TEI-XML/Amtsblatt/' # Directory containing the original files\n",
    "output_dir = 'test_data/postprocessed/' # Output directory for the merged files generated by this script\n",
    "output_dir_diff_files = 'diffs' # Output directory for the diff files\n",
    "\n",
    "# Should the postprocessed XML be validated with the TEI schema (takes a long time)?\n",
    "XML_VALIDATION_ACTIV = False\n",
    "\n",
    "\n",
    "def is_nested(entity):\n",
    "    \"\"\" Check if the current entity is nested within another entity.\n",
    "\n",
    "    Args:\n",
    "        entity (BeautifulSoup Tag): The entity (e.g., <placeName>, <persName>, <orgName>).\n",
    "\n",
    "    Returns:\n",
    "        bool: True if the entity is nested within another entity, False otherwise.\n",
    "    \"\"\"\n",
    "    \n",
    "    parent = entity.find_parent()\n",
    "    while parent:\n",
    "        if parent.name in {'placeName', 'persName', 'orgName'} and parent != entity:\n",
    "            return True\n",
    "        parent = parent.find_parent()\n",
    "    return False\n",
    "\n",
    "\n",
    "def filter_nested_entities(entities):\n",
    "    \"\"\" Remove all nested entities from the list that are already contained within a parent entity\n",
    "\n",
    "    Args:\n",
    "        entities (list of BeautifulSoup Tags): List of entity tags from the edited XML.\n",
    "\n",
    "    Returns:\n",
    "        list of BeautifulSoup Tags: List of non-nested entities.\n",
    "    \"\"\"\n",
    "    \n",
    "    non_nested_entities = []\n",
    "\n",
    "    for entity in entities:\n",
    "        if not is_nested(entity):\n",
    "            non_nested_entities.append(entity)\n",
    "\n",
    "    return non_nested_entities\n",
    "\n",
    "\n",
    "def count_entities(xml_text):\n",
    "    \"\"\" Count the number of TEI entity tags (placeName, persName, orgName) in the given XML text.\n",
    "    \n",
    "    Args:\n",
    "        xml_text (str): The input text containing XML data.\n",
    "    \n",
    "    Returns:\n",
    "        int: The total count of TEI entities.\n",
    "    \"\"\"\n",
    "    \n",
    "    total_count  = 0\n",
    "\n",
    "    tag_patterns = {\n",
    "        'placeName': r'</?placeName[^>]*>',\n",
    "        'persName': r'</?persName[^>]*>',\n",
    "        'orgName': r'</?orgName[^>]*>'\n",
    "    }\n",
    "\n",
    "    # Count the number of occurrences of each pattern in the text\n",
    "    for pattern in tag_patterns.values():\n",
    "        total_count  += len(re.findall(pattern, xml_text))\n",
    "\n",
    "    # Divide by 2, as there is always a start and an end tag. Start and end tags are counted to detect if either is missing.\n",
    "    return (total_count//2)\n",
    "    \n",
    "\n",
    "def remove_entity_tags_in_str(text):\n",
    "    \"\"\" Remove all TEI entity tags (placeName, persName, orgName) from a string.\n",
    "\n",
    "    Args:\n",
    "        text (str): The input string that may contain TEI entity tags.\n",
    "\n",
    "    Returns:\n",
    "        str: The input string with all TEI entity tags removed.\n",
    "    \"\"\"\n",
    "    \n",
    "    tag_patterns = {\n",
    "        'placeName': r'</?placeName[^>]*>',\n",
    "        'persName': r'</?persName[^>]*>',\n",
    "        'orgName': r'</?orgName[^>]*>'\n",
    "    }\n",
    "    \n",
    "    # Remove the tags for each entity type\n",
    "    for tag_name, pattern in tag_patterns.items():\n",
    "        text = re.sub(pattern, '', text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "\n",
    "def get_text_for_lookbehind(entity, lookbehind_length, removeEntityTags = True):\n",
    "    \"\"\" Extract up to 30 characters of text before the entity, used for regex lookbehind.\n",
    "\n",
    "    Args:\n",
    "        entity (BeautifulSoup Tag): The entity tag from which to extract the lookbehind text.\n",
    "        removeEntityTags (bool, optional): Whether to remove nested entity tags from the lookbehind text. Defaults to True.\n",
    "        lookbehind_length (int): The desired length of the returned lookbehind text\n",
    "\n",
    "    Returns:\n",
    "        str: The 30 characters (if removeEntityTags is false) or 20 characters (otherwise) before the entity in the parent element.\n",
    "    \"\"\"\n",
    "    \n",
    "    parent_element = entity.find_parent()\n",
    "    \n",
    "    if parent_element:\n",
    "        parent_text = ''.join(str(content) for content in parent_element.contents)\n",
    "        entity_str = str(entity)\n",
    "        index_of_child = parent_text.find(entity_str)\n",
    "        text_before_child_with_entities = parent_text[:index_of_child]\n",
    "\n",
    "        if not removeEntityTags:\n",
    "            # return 20 characters before entity tag as lookbehind text\n",
    "            #print(entity, \"text_before_child_with_entities:\", text_before_child_with_entities[-lookbehind_length:])\n",
    "            return text_before_child_with_entities[-lookbehind_length:]\n",
    "\n",
    "        text_before_child_without_entities = remove_entity_tags_in_str(text_before_child_with_entities)\n",
    "\n",
    "        # return 30 characters before entity tag as lookbehind text\n",
    "        #print(entity, \"text_before_child_without_entities:\", text_before_child_without_entities[-lookbehind_length:])\n",
    "        return text_before_child_without_entities[-lookbehind_length:]\n",
    "\n",
    "    return \"\"\n",
    "    \n",
    "\n",
    "def prepare_search_text(entity):\n",
    "    \"\"\" Prepare the search text by removing all entity tags from the entity. \n",
    "        This will make the text match the text in the original file (before NER).\n",
    "\n",
    "    Args:\n",
    "        entity (BeautifulSoup Tag): The entity whose text is being prepared.\n",
    "\n",
    "    Returns:\n",
    "        str: The cleaned entity text, without any nested tags.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Remove all nested entities inside the current entity\n",
    "    for inner_entity in entity.find_all(['placeName', 'persName', 'orgName']):\n",
    "        inner_entity.unwrap()  # Removes the tag but retains the content\n",
    "\n",
    "    # Also remove the parent entity to prepare the text for search\n",
    "    search_text = ''.join(str(content) for content in entity.contents)\n",
    "    \n",
    "    return search_text\n",
    "\n",
    "    \n",
    "def insert_done_in_every_word(sentence):\n",
    "    \"\"\" Insert the marker `---DONE---` into each word of the replacement string\n",
    "    to prevent repeated matches during the search-and-replace process.\n",
    "\n",
    "    Args:\n",
    "        sentence (str): The sentence or entity text in which to insert the marker.\n",
    "\n",
    "    Returns:\n",
    "        str: The modified sentence with `---DONE---` inserted.\n",
    "    \"\"\"\n",
    "    \n",
    "    modified_words = []\n",
    "\n",
    "    pattern = re.compile(r'(<[^>]*>| )')\n",
    "    words = pattern.split(sentence)\n",
    "    \n",
    "    for word in words:\n",
    "        modified_word = word[:len(word)//2] + \"---DONE---\" + word[len(word)//2:]\n",
    "        modified_words.append(modified_word)\n",
    "    \n",
    "    modified_sentence = ''.join(modified_words)\n",
    "    \n",
    "    return modified_sentence\n",
    "\n",
    "\n",
    "def validate_result(filename, original_xml, edited_xml, postprocessed_xml):\n",
    "    \"\"\" Validates the postprocessing results by comparing the original, edited, and post-processed files,\n",
    "    generating an HTML diff, and validating the XML syntax (optional).\n",
    "\n",
    "    Args:\n",
    "        filename (str): Name of the file being processed.\n",
    "        original_xml (str): The original XML content as a string.\n",
    "        edited_xml (str): The XML content after NER (named entity recognition) processing.\n",
    "        postprocessed_xml (str): The final XML content after post-processing.\n",
    "    \"\"\"\n",
    "\n",
    "    def sort_attributes_in_xml(xml):\n",
    "        \"\"\" Recursively sort attributes of an XML element and its children.\n",
    "\n",
    "        Args:\n",
    "            element (xml.etree.ElementTree.Element): The XML element whose attributes need sorting.\n",
    "        \"\"\"\n",
    "        root = ET.fromstring(xml)\n",
    "    \n",
    "        def sort_attributes(elem):\n",
    "            elem.attrib = dict(sorted(elem.attrib.items()))\n",
    "            \n",
    "            for child in elem:\n",
    "                sort_attributes(child)\n",
    "    \n",
    "        sort_attributes(root)\n",
    "\n",
    "        return root\n",
    "\n",
    "\n",
    "    def normalize_xml(xml_string):\n",
    "        \"\"\" Normalize an XML string by sorting element attributes and removing excess whitespace/tab.\n",
    "\n",
    "        Args:\n",
    "            xml_string (str): The XML content as a string.\n",
    "\n",
    "        Returns:\n",
    "            str: The normalized XML string with sorted attributes and no unnecessary whitespaces/tabs.\n",
    "        \"\"\"\n",
    "\n",
    "        xml_sorted = sort_attributes_in_xml(xml_string)\n",
    "    \n",
    "        # Remove unnecessary whitespaces, tabs and newlines\n",
    "        normalized_string = ET.tostring(xml_sorted, encoding='utf-8').decode('utf-8')\n",
    "        normalized_string = re.sub(r\">\\s+<\", \"><\", normalized_string)\n",
    "        normalized_string = re.sub(r\"\\s+\", \" \", normalized_string)\n",
    "        normalized_string = re.sub(r\"[\\t\\n]+\", \" \", normalized_string)\n",
    "        \n",
    "        return normalized_string\n",
    "\n",
    "    \n",
    "    \n",
    "    def compare_xml_strings(xml_string1, xml_string2):\n",
    "        \"\"\" Compare two XML strings after normalizing them. Updates the statistics log with \n",
    "        whether the content matches.\n",
    "\n",
    "        Args:\n",
    "            xml1 (str): First XML string to compare.\n",
    "            xml2 (str): Second XML string to compare.\n",
    "        \"\"\"\n",
    "        \n",
    "        normalized_xml1 = normalize_xml(xml_string1)\n",
    "        normalized_xml2 = normalize_xml(xml_string2)\n",
    "    \n",
    "        if normalized_xml1 == normalized_xml2:\n",
    "            stats.write_to_statistics(filename,'Content integrity (Original <-> Postprocessed)', 'Yes')\n",
    "        else:\n",
    "            stats.write_to_statistics(filename,'Content integrity (Original <-> Postprocessed)', 'No')\n",
    "\n",
    "    compare_xml_strings(original_xml, remove_entity_tags_in_str(postprocessed_xml))\n",
    "\n",
    "\n",
    "    def show_differences(text1, text2, text1_description, text2_description):\n",
    "        \"\"\" Display the differences between two XML strings and save the result in an HTML file.\n",
    "\n",
    "        Args:\n",
    "            text1 (str): First text to compare.\n",
    "            text2 (str): Second text to compare.\n",
    "            text1_description (str): Description of the first text, used in the diff output.\n",
    "            text2_description (str): Description of the second text, used in the diff output.\n",
    "        \"\"\"\n",
    "\n",
    "        def prepare_for_diff(xml_string):\n",
    "            \"\"\"\n",
    "            Prepare an XML string for diff by sorting attributes and removing unnecessary whitespaces.\n",
    "\n",
    "            Args:\n",
    "                xml_string (str): The XML string to prepare for comparison.\n",
    "\n",
    "            Returns:\n",
    "                str: A cleaned and sorted XML string ready for diff comparison.\n",
    "            \"\"\"\n",
    "\n",
    "            xml_sorted = sort_attributes_in_xml(xml_string)\n",
    "            \n",
    "            xml_string = ET.tostring(xml_sorted, encoding='utf-8', method='xml').decode('utf-8')\n",
    "\n",
    "            # Remove tabs, leading whitespaces, and namespace prefixes for cleaner comparison\n",
    "            xml_string = re.sub(r\"[\\t]+\", \" \", xml_string)  # Remove tabs\n",
    "            xml_string = re.sub(r'^\\s+', '', xml_string, flags=re.MULTILINE)  # Remove whitespaces at the beginning \n",
    "            xml_string = re.sub(\"ns0:\", \"\", xml_string)\n",
    "            \n",
    "            return xml_string\n",
    "        \n",
    "        text1 = prepare_for_diff(text1)\n",
    "        text2 = prepare_for_diff(text2)\n",
    "        \n",
    "\n",
    "        def save_html_diff(filename, text1, text2, text1_description, text2_description):\n",
    "            \"\"\" Create and save an HTML file showing the diff between two texts.\n",
    "\n",
    "            Args:\n",
    "                filename (str): The name of the file being compared.\n",
    "                text1 (str): First text to compare.\n",
    "                text2 (str): Second text to compare.\n",
    "                text1_description (str): Short description of the first text.\n",
    "                text2_description (str): Short description of the second text.\n",
    "            \"\"\"\n",
    "            \n",
    "            differ = difflib.HtmlDiff()\n",
    "            html_diff = differ.make_file(text1.splitlines(), text2.splitlines(), fromdesc=text1_description, todesc=text2_description)\n",
    "            \n",
    "            # Ensure the output directory exists, if not create it\n",
    "            if not os.path.exists(output_dir_diff_files):\n",
    "                os.makedirs(output_dir_diff_files)\n",
    "\n",
    "            html_filename = f'diff_{os.path.splitext(filename)[0]}.html'\n",
    "\n",
    "            diff_path = os.path.join(f'{output_dir_diff_files}/', html_filename)\n",
    "          \n",
    "            # Write the HTML diff file to disk\n",
    "            with open(diff_path, 'w', encoding='utf-8') as f:\n",
    "                f.write(html_diff)\n",
    "            \n",
    "            print(f\"HTML-Diff saved in {diff_path}\")\n",
    "\n",
    "\n",
    "        save_html_diff(filename, text1, text2, text1_description, text2_description)\n",
    "\n",
    "    show_differences(edited_xml, postprocessed_xml, \"NER-processed\", \"Postprocessed\")\n",
    "        \n",
    "\n",
    "    def validate_xml(filename, xml_file, xsd_file):\n",
    "        \"\"\" Validate the XML against the TEI schema and log the result.\n",
    "\n",
    "        Args:\n",
    "            filename (str): The name of the file being validated.\n",
    "            xml_file (str): The XML file content to validate.\n",
    "            xsd_file (str): The XSD schema file to validate against.\n",
    "        \"\"\"\n",
    "        \n",
    "        print(\"XML Validation in progress...\")\n",
    "        schema = xmlschema.XMLSchema(xsd_file)\n",
    "        \n",
    "        try:\n",
    "            is_valid = schema.is_valid(xml_file)\n",
    "            if is_valid:\n",
    "                stats.write_to_statistics(filename,'XML (TEI Schema) valid', \"Yes\")\n",
    "            else:\n",
    "                stats.write_to_statistics(filename,'XML (TEI Schema) valid', \"No\")\n",
    "                print(f\"{filename} is invalid.\")\n",
    "        except Exception as err:\n",
    "            stats.write_to_statistics(filename,'XML (TEI Schema) valid', \"No\")\n",
    "            print(f\"{filename} is invalid. Error: \")\n",
    "            print(\"Error: \", err)\n",
    "\n",
    "    if XML_VALIDATION_ACTIV:\n",
    "        validate_xml(filename, postprocessed_xml, 'tei_schemas/tei_all.xsd')\n",
    "\n",
    "\n",
    "    # Log the number of entities before and after post-processing\n",
    "    entities_before_postprocessing = count_entities(edited_xml)\n",
    "    stats.write_to_statistics(filename,'Number of Entities Before Processing', entities_before_postprocessing)\n",
    "    entities_after_postprocessing = count_entities(postprocessed_xml)\n",
    "    stats.write_to_statistics(filename,'Number of Entities After Processing', entities_after_postprocessing)\n",
    "\n",
    "    # Calculate and log any missing entities after processing\n",
    "    missing_entities = entities_before_postprocessing - entities_after_postprocessing\n",
    "    stats.write_to_statistics(filename,'Missing Entities', missing_entities)\n",
    "\n",
    "    \n",
    "def merge_entities(filename, original_xml, edited_xml):\n",
    "    \"\"\" Merge the named entities from the edited XML into the original XML (only within <body>).\n",
    "\n",
    "    Args:\n",
    "        original_xml (str): The XML content of the original file.\n",
    "        edited_xml (str): The XML content of the NER-processed file with errors.\n",
    "\n",
    "    Returns:\n",
    "        str: The original XML content with the corrected entities inserted.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Parse the original and edited XML\n",
    "    original_soup = BeautifulSoup(original_xml, 'xml')\n",
    "    edited_soup = BeautifulSoup(edited_xml, 'xml')\n",
    "\n",
    "    # Extract <body> content from both documents\n",
    "    original_body = original_soup.find('body')\n",
    "    edited_body = edited_soup.find('body')\n",
    "\n",
    "    # Ensure <body> exists in both documents\n",
    "    if original_body and edited_body:\n",
    "        \n",
    "        # Find all entities in the edited XML (places, people, organizations)\n",
    "        entities = edited_body.find_all(['placeName', 'persName', 'orgName'])\n",
    "\n",
    "        # Remove all entities that are already nested within another entity\n",
    "        non_nested_entities = filter_nested_entities(entities)\n",
    "\n",
    "        # List to store entities that couldn't be replaced in the round one\n",
    "        unreplaced_entities = []\n",
    "        \n",
    "        original_body_str = str(original_body)\n",
    "\n",
    "        # Round one using greater context (lookbehind) and ignoring entity tags\n",
    "        def insert_entities_first_round(unreplaced_entities, original_body_str, lookbehind_length):\n",
    "            for entity in non_nested_entities:\n",
    "                \n",
    "                searchText = prepare_search_text(copy.deepcopy(entity))\n",
    "                text_for_lookbehind = get_text_for_lookbehind(entity, lookbehind_length)\n",
    "                \n",
    "                # Insert \"---DONE---\" in replacement text to prevent re-matching\n",
    "                replaceText = insert_done_in_every_word(str(entity))\n",
    "    \n",
    "                # Create the regex pattern for contextual replacement\n",
    "                context_pattern = (\n",
    "                    r'(?<=' + re.escape(text_for_lookbehind) + r')\\s*' + re.escape(searchText)\n",
    "                )\n",
    "                \n",
    "                # Perform the replacement if the lookbehind is found\n",
    "                original_body_str, count = re.subn(context_pattern, replaceText, original_body_str, count=1)\n",
    "    \n",
    "                if count == 0:\n",
    "                    # If no replacements were made, add the entity to the unreplaced list\n",
    "                    unreplaced_entities.append(entity)\n",
    "\n",
    "        unreplaced_entities, original_body_str = insert_entities_first_round(non_nested_entities, original_body_str, lookbehind_length = 40)\n",
    "        unreplaced_entities, original_body_str = insert_entities_first_round(non_nested_entities, original_body_str, lookbehind_length = 30)\n",
    "\n",
    "        # Remove \"---DONE---\" markers\n",
    "        original_body_str = original_body_str.replace(\"---DONE---\", \"\")\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "        # Round two uses a shorter context and considers the presence of entity tags. \n",
    "        # A possible reason for the previous lookbehind not matching could be that an entity \n",
    "        # has already been inserted into the original file, causing the current lookbehind \n",
    "        # to fail when trying to match with the next entity's surrounding text.\n",
    "\n",
    "        def insert_entities_second_round(unreplaced_entities, original_body_str, lookbehind_length):\n",
    "            unreplaced_entities_after_round_two = []\n",
    "            \n",
    "            for entity in unreplaced_entities:\n",
    "                \n",
    "                searchText = prepare_search_text(copy.deepcopy(entity))\n",
    "                text_for_lookbehind = get_text_for_lookbehind(entity, lookbehind_length, removeEntityTags = False)\n",
    "                \n",
    "                # Do not insert \"---DONE---\" in this round. The purpose of the second round is to find entities \n",
    "                # that were missed in the first round due to already inserted entity tags in the original document.\n",
    "                replaceText = str(entity)\n",
    "    \n",
    "                context_pattern = (\n",
    "                    r'(?<=' + re.escape(text_for_lookbehind) + r')\\s*' + re.escape(searchText)\n",
    "                )\n",
    "    \n",
    "                # Attempt the replacement\n",
    "                original_body_str, count = re.subn(context_pattern, replaceText, original_body_str, count=1)\n",
    "    \n",
    "                if count == 0:\n",
    "                    # If no replacements were made, add the entity to the unreplaced list\n",
    "                    unreplaced_entities_after_round_two.append(entity)\n",
    "                    \n",
    "            return unreplaced_entities_after_round_two, original_body_str\n",
    "\n",
    "        unreplaced_entities, original_body_str = insert_entities_second_round(unreplaced_entities, original_body_str, lookbehind_length = 30)\n",
    "        unreplaced_entities, original_body_str = insert_entities_second_round(unreplaced_entities, original_body_str, lookbehind_length = 20)\n",
    "\n",
    "\n",
    "        stats.write_to_statistics(filename,'Entities failed', unreplaced_entities)\n",
    "\n",
    "        '''\n",
    "        unreplaced_entities_after_round_three = []\n",
    "\n",
    "        #print(original_body_str)\n",
    "\n",
    "        # Last round: Try without context\n",
    "        for entity in unreplaced_entities_after_round_two:\n",
    "            \n",
    "            searchText = prepare_search_text(copy.deepcopy(entity))\n",
    "            \n",
    "            # Do not insert \"---DONE---\" in this round. The purpose of the second round is to find entities \n",
    "            # that were missed in the first round due to already inserted entity tags in the original document.\n",
    "            replaceText = str(entity)\n",
    "\n",
    "            \n",
    "            # Attempt the replacement\n",
    "            original_body_str, count = re.subn(searchText, replaceText, original_body_str, count=1)\n",
    "\n",
    "            if count == 0:\n",
    "                # If no replacements were made, add the entity to the unreplaced list\n",
    "                unreplaced_entities_after_round_three.append(entity)\n",
    "                \n",
    "        stats.write_to_statistics(filename,'Entities failed (3rd round)', unreplaced_entities_after_round_three)\n",
    "        '''\n",
    "\n",
    "        \n",
    "        # Replace the old <body> with the modified version in the original document\n",
    "        original_body.replace_with(BeautifulSoup(original_body_str, 'xml').body)\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "    return str(original_soup)\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\" Process XML documents by reading, merging entities, and saving the results.\n",
    "\n",
    "    This function reads NER-processed XML files,\n",
    "    merges the identified entities into the corresponding original files,\n",
    "    and then saves the merged output into a new directory.\n",
    "    \"\"\"\n",
    "\n",
    "    # Ensure the output directory exists, create it if necessary\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    \n",
    "    for filename in os.listdir(edited_dir):\n",
    "        \n",
    "        if filename.endswith('.xml'):\n",
    "\n",
    "            # Read edited and original xml file\n",
    "            edited_file_path = os.path.join(edited_dir, filename)\n",
    "            original_file_path = os.path.join(original_dir, filename)\n",
    "    \n",
    "            with open(edited_file_path, 'r', encoding='utf-8') as file:\n",
    "                edited_xml = file.read()\n",
    "    \n",
    "            if os.path.exists(original_file_path): \n",
    "                with open(original_file_path, 'r', encoding='utf-8') as file:\n",
    "                    original_xml = file.read()\n",
    "            else:\n",
    "                print(f\"Original file not found for {filename}\")\n",
    "                continue\n",
    "    \n",
    "            # Merge entities from the edited file into the original file\n",
    "            result = merge_entities(filename, original_xml, edited_xml)           \n",
    "\n",
    "            # Validate result\n",
    "            validate_result(filename, original_xml, edited_xml, result)\n",
    "    \n",
    "            # Save the merged result to the output directory as an XML document\n",
    "            output_file_path = os.path.join(output_dir, filename)\n",
    "            with open(output_file_path, 'w', encoding='utf-8') as output_file:\n",
    "                output_file.write(result)\n",
    "\n",
    "            stats.save_statistics_to_csv()\n",
    "            print(f\"Processed and saved {filename} to {output_file_path}\\n\")\n",
    "\n",
    "    print(f\"All files processed\")\n",
    "\n",
    "# Let the magic happen\n",
    "main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d613a6f-a4f8-4da7-b741-142251e7bb83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib  # Importiere importlib\n",
    "import stats  # Importiere dein Modul\n",
    "\n",
    "# Änderungen an my_module vornehmen...\n",
    "\n",
    "# Jetzt das Modul neu laden\n",
    "importlib.reload(stats)  # Lade das Modul neu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e12137-12e4-4112-bb73-0f7b1ff89793",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install xmlschema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb64c0ae-7fc7-4ed1-9ac9-c8f8d7fb6988",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import xmlschema\n",
    "\n",
    "with open(\"test_data/postprocessed/ABl_1980__S__1008-1016_.xml\", 'r', encoding='utf-8') as file:\n",
    "    xml_file = file.read()\n",
    "\n",
    "def validate_xml(xml_file, xsd_file):\n",
    "        # Lade das XSD-Schema\n",
    "        print(xsd_file)\n",
    "\n",
    "        schema_file = open(xsd_file)\n",
    "        schema = xmlschema.XMLSchema(schema_file)\n",
    "\n",
    "        # Validieren des XML-Dokuments\n",
    "        try:\n",
    "            is_valid = schema.is_valid(xml_file)\n",
    "            if is_valid:\n",
    "                print(f\"File ist gültig.\")\n",
    "            else:\n",
    "                print(f\"File ist ungültig. Fehler:\")\n",
    "        except Exception as err:\n",
    "            print(f\"File ist ungültig. Fehler:\")\n",
    "            print(\"Error: \", err)\n",
    "            \n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "# Beispielaufruf der Funktion\n",
    "validate_xml(xml_file, 'tei_all.xsd')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "380d66c0-c56f-42cc-af16-665d510e8662",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
